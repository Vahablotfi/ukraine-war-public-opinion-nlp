{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0c0e63-4f80-43bb-89be-556151f8be3e",
   "metadata": {},
   "source": [
    "# Event Extraction (Chunked Processing)\n",
    "\n",
    "This notebook takes the full raw Kaggle dataset (`reddit_opinion_ru_ua.csv`, ~5GB) and extracts\n",
    "five smaller event-based datasets for analysis.\n",
    "\n",
    "Goals:\n",
    "- Avoid memory issues by using chunked loading\n",
    "- Filter comments into predefined event windows\n",
    "- Keep **all** subreddits but label whether each comment is from a U.S.-focused subreddit\n",
    "- Save one CSV per event for fast downstream analysis (sentiment + topics + Streamlit)\n",
    "\n",
    "Outputs (created locally):\n",
    "- `data/processed/event1_kyiv.csv`\n",
    "- `data/processed/event2_kherson.csv`\n",
    "- `data/processed/event3_stalemate.csv`\n",
    "- `data/processed/event4_trump_election.csv`\n",
    "- `data/processed/event5_white_house_meeting.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ed7e9-a1c0-4db9-9a30-41746c21a3e0",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "We load required libraries and define file paths.\n",
    "\n",
    "Because the source file is very large, we will read it in chunks using pandas `chunksize`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aef2de09-2e28-44c3-9d9a-52eb75f83ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92619982-6f02-4afe-8c45-5c3fca6621ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (raw 5GB Kaggle file)\n",
    "file_path = \"../data/raw/reddit_opinion_ru_ua.csv\"\n",
    "\n",
    "# Output directory (event files will be created here)\n",
    "output_dir = \"../data/processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Chunk size (rows per chunk)\n",
    "chunksize = 200_000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1b62f1-f230-4f22-97ad-87bb253936fd",
   "metadata": {},
   "source": [
    "## 2. Define Event Windows\n",
    "\n",
    "These are the five time windows we will extract from the full dataset.\n",
    "Each window represents a major moment in the Ukraine war or U.S. political narrative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3a0334f-008c-4fe0-add8-4ba5b7ee0559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'event1_kyiv': (Timestamp('2022-02-20 00:00:00'),\n",
       "  Timestamp('2022-03-20 00:00:00')),\n",
       " 'event2_kherson': (Timestamp('2022-10-15 00:00:00'),\n",
       "  Timestamp('2022-12-01 00:00:00')),\n",
       " 'event3_stalemate': (Timestamp('2023-03-01 00:00:00'),\n",
       "  Timestamp('2023-06-30 00:00:00')),\n",
       " 'event4_trump_election': (Timestamp('2024-10-25 00:00:00'),\n",
       "  Timestamp('2024-11-20 00:00:00')),\n",
       " 'event5_white_house_meeting': (Timestamp('2025-02-15 00:00:00'),\n",
       "  Timestamp('2025-03-10 00:00:00'))}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_windows = {\n",
    "    \"event1_kyiv\": (\"2022-02-20\", \"2022-03-20\"),\n",
    "    \"event2_kherson\": (\"2022-10-15\", \"2022-12-01\"),\n",
    "    \"event3_stalemate\": (\"2023-03-01\", \"2023-06-30\"),\n",
    "    \"event4_trump_election\": (\"2024-10-25\", \"2024-11-20\"),\n",
    "    \"event5_white_house_meeting\": (\"2025-02-15\", \"2025-03-10\"),\n",
    "}\n",
    "\n",
    "# Convert to datetime for comparisons\n",
    "event_windows = {k: (pd.to_datetime(v[0]), pd.to_datetime(v[1])) for k, v in event_windows.items()}\n",
    "event_windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b22f7e-6186-4228-b91c-990d8beaa7c0",
   "metadata": {},
   "source": [
    "## 3. Subreddit Strategy (U.S.-focused vs Non-U.S.)\n",
    "\n",
    "Reddit does not provide verified user location. To approximate U.S.-focused public opinion,\n",
    "we label comments from subreddits that are strongly U.S.-centric (e.g., U.S. politics communities).\n",
    "\n",
    "Important:\n",
    "- We do **not** drop non-U.S. subreddits here.\n",
    "- We keep all data and add a boolean column: `is_us_focused`.\n",
    "- Downstream analysis can focus on `is_us_focused == True` while still allowing comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b1b24e7-787b-4f7d-9cea-5fc67b994e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AskReddit',\n",
       " 'Conservative',\n",
       " 'WayOfTheBern',\n",
       " 'geopolitics',\n",
       " 'neoliberal',\n",
       " 'news',\n",
       " 'politics',\n",
       " 'worldnews'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_focused_subreddits = {\n",
    "    \"politics\",\n",
    "    \"Conservative\",\n",
    "    \"neoliberal\",\n",
    "    \"WayOfTheBern\",\n",
    "    \"AskReddit\",\n",
    "    \"news\",\n",
    "    \"worldnews\",\n",
    "    \"geopolitics\",\n",
    "}\n",
    "\n",
    "us_focused_subreddits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bdf0b5-fb05-4ea1-998e-9ec4c563cb42",
   "metadata": {},
   "source": [
    "## 4. Columns to Keep\n",
    "\n",
    "The raw dataset contains many columns. We only keep what we need for analysis and reporting.\n",
    "\n",
    "Minimum needed:\n",
    "- comment timestamp (`created_time`)\n",
    "- text (`self_text`)\n",
    "- subreddit (`subreddit`)\n",
    "\n",
    "Helpful metadata (if available):\n",
    "- comment score, post title, post timestamp, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f7f2741-d654-4ed3-ad3d-2eeddc6b1612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comment_id',\n",
       " 'created_time',\n",
       " 'self_text',\n",
       " 'subreddit',\n",
       " 'score',\n",
       " 'post_id',\n",
       " 'post_title',\n",
       " 'post_self_text',\n",
       " 'post_created_time',\n",
       " 'post_score']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_keep = [\n",
    "    \"comment_id\",\n",
    "    \"created_time\",\n",
    "    \"self_text\",\n",
    "    \"subreddit\",\n",
    "    \"score\",\n",
    "    \"post_id\",\n",
    "    \"post_title\",\n",
    "    \"post_self_text\",\n",
    "    \"post_created_time\",\n",
    "    \"post_score\",\n",
    "]\n",
    "\n",
    "cols_to_keep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b9f3b-d7ba-46a6-99c4-7e490f159607",
   "metadata": {},
   "source": [
    "## 5. Cleaning Rules\n",
    "\n",
    "This extraction step performs light cleaning:\n",
    "- Drop rows with missing text or timestamp\n",
    "- Remove obvious non-content placeholders: `[deleted]`, `[removed]`\n",
    "- Add:\n",
    "  - `dt` parsed timestamp column\n",
    "  - `is_us_focused` boolean label for subreddit group\n",
    "\n",
    "Heavy cleaning (tokenization, lemmatization, etc.) will happen in later notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b8c83a0-e2d8-47c9-ad9c-7597cae801bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_text(s: str) -> bool:\n",
    "    \"\"\"Return True if text looks like a real comment (not empty/removed/deleted).\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return False\n",
    "    s = s.strip()\n",
    "    if len(s) == 0:\n",
    "        return False\n",
    "    if s.lower() in {\"[deleted]\", \"[removed]\"}:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e50d0-934b-483e-9e7e-cd0f64e6b565",
   "metadata": {},
   "source": [
    "## 6. Chunked Extraction Loop\n",
    "\n",
    "We iterate through the raw CSV in chunks and:\n",
    "- keep selected columns\n",
    "- parse timestamps\n",
    "- label U.S.-focused subreddits\n",
    "- filter each chunk into the relevant event windows\n",
    "- append results to event CSVs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e497dc9-e77a-4e70-981c-d39fac8dad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files will be created here:\n",
      "event1_kyiv -> ../data/processed/event1_kyiv.csv\n",
      "event2_kherson -> ../data/processed/event2_kherson.csv\n",
      "event3_stalemate -> ../data/processed/event3_stalemate.csv\n",
      "event4_trump_election -> ../data/processed/event4_trump_election.csv\n",
      "event5_white_house_meeting -> ../data/processed/event5_white_house_meeting.csv\n",
      "\n",
      "Processing chunk 0 ...\n",
      "Progress so far: {}\n",
      "\n",
      "Processing chunk 1 ...\n",
      "\n",
      "Processing chunk 2 ...\n",
      "\n",
      "Processing chunk 3 ...\n",
      "\n",
      "Processing chunk 4 ...\n",
      "\n",
      "Processing chunk 5 ...\n",
      "Progress so far: {'event5_white_house_meeting': 65928}\n",
      "\n",
      "Processing chunk 6 ...\n",
      "\n",
      "Processing chunk 7 ...\n",
      "\n",
      "Processing chunk 8 ...\n",
      "\n",
      "Processing chunk 9 ...\n",
      "\n",
      "Processing chunk 10 ...\n",
      "Progress so far: {'event5_white_house_meeting': 241262, 'event4_trump_election': 47653}\n",
      "\n",
      "Processing chunk 11 ...\n",
      "\n",
      "Processing chunk 12 ...\n",
      "\n",
      "Processing chunk 13 ...\n",
      "\n",
      "Processing chunk 14 ...\n",
      "\n",
      "Processing chunk 15 ...\n",
      "Progress so far: {'event5_white_house_meeting': 241262, 'event4_trump_election': 359088}\n",
      "\n",
      "Processing chunk 16 ...\n",
      "\n",
      "Processing chunk 17 ...\n",
      "\n",
      "Processing chunk 18 ...\n",
      "\n",
      "Processing chunk 19 ...\n",
      "\n",
      "Processing chunk 20 ...\n",
      "Progress so far: {'event5_white_house_meeting': 241262, 'event4_trump_election': 359088}\n",
      "\n",
      "Processing chunk 21 ...\n",
      "\n",
      "Processing chunk 22 ...\n",
      "\n",
      "Processing chunk 23 ...\n",
      "\n",
      "Processing chunk 24 ...\n",
      "\n",
      "Processing chunk 25 ...\n",
      "Progress so far: {'event5_white_house_meeting': 241262, 'event4_trump_election': 359088}\n",
      "\n",
      "Processing chunk 26 ...\n",
      "\n",
      "Processing chunk 27 ...\n",
      "\n",
      "Processing chunk 28 ...\n",
      "\n",
      "Processing chunk 29 ...\n",
      "\n",
      "Done.\n",
      "Final rows written per event:\n",
      "event1_kyiv 2940\n",
      "event2_kherson 795\n",
      "event3_stalemate 9955\n",
      "event4_trump_election 359088\n",
      "event5_white_house_meeting 241262\n"
     ]
    }
   ],
   "source": [
    "# Track how many rows written per event (for summary)\n",
    "written_counts = defaultdict(int)\n",
    "\n",
    "# Output paths for each event\n",
    "event_paths = {event: os.path.join(output_dir, f\"{event}.csv\") for event in event_windows.keys()}\n",
    "\n",
    "# Remove old outputs if they already exist (optional safety)\n",
    "for p in event_paths.values():\n",
    "    if os.path.exists(p):\n",
    "        os.remove(p)\n",
    "\n",
    "print(\"Output files will be created here:\")\n",
    "for event, p in event_paths.items():\n",
    "    print(event, \"->\", p)\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunksize, low_memory=False, on_bad_lines=\"skip\")):\n",
    "    print(f\"\\nProcessing chunk {i} ...\")\n",
    "\n",
    "    # Keep only columns that exist in this chunk (robustness)\n",
    "    existing_cols = [c for c in cols_to_keep if c in chunk.columns]\n",
    "    chunk = chunk[existing_cols].copy()\n",
    "\n",
    "    # Parse datetime\n",
    "    chunk[\"dt\"] = pd.to_datetime(chunk[\"created_time\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop invalid dt or missing text/subreddit\n",
    "    if \"self_text\" in chunk.columns:\n",
    "        chunk = chunk[chunk[\"self_text\"].apply(is_valid_text)]\n",
    "    chunk = chunk.dropna(subset=[\"dt\", \"subreddit\"])\n",
    "\n",
    "    # Label U.S.-focused subreddits\n",
    "    chunk[\"is_us_focused\"] = chunk[\"subreddit\"].isin(us_focused_subreddits)\n",
    "\n",
    "    # For each event window, filter and append\n",
    "    for event, (start, end) in event_windows.items():\n",
    "        mask = (chunk[\"dt\"] >= start) & (chunk[\"dt\"] <= end)\n",
    "        event_chunk = chunk.loc[mask].copy()\n",
    "\n",
    "        if event_chunk.empty:\n",
    "            continue\n",
    "\n",
    "        out_path = event_paths[event]\n",
    "        write_header = not os.path.exists(out_path)\n",
    "\n",
    "        event_chunk.to_csv(out_path, mode=\"a\", header=write_header, index=False)\n",
    "        written_counts[event] += len(event_chunk)\n",
    "\n",
    "    # Print progress every few chunks\n",
    "    if i % 5 == 0:\n",
    "        print(\"Progress so far:\", dict(written_counts))\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(\"Final rows written per event:\")\n",
    "for event in event_windows.keys():\n",
    "    print(event, written_counts[event])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846d624-8251-4154-9afc-8bb51760325e",
   "metadata": {},
   "source": [
    "## 7. Validate Extracted Event Files\n",
    "\n",
    "We now load each event CSV and compute basic statistics:\n",
    "- total rows\n",
    "- min/max date\n",
    "- percent U.S.-focused\n",
    "- top subreddits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47ab86dd-11ec-45fc-a7fe-241145624ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_event_csv(path: str, top_n: int = 10):\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df[\"dt\"] = pd.to_datetime(df[\"created_time\"], errors=\"coerce\")\n",
    "\n",
    "    total = len(df)\n",
    "    dt_min = df[\"dt\"].min()\n",
    "    dt_max = df[\"dt\"].max()\n",
    "\n",
    "    us_pct = None\n",
    "    if \"is_us_focused\" in df.columns:\n",
    "        us_pct = df[\"is_us_focused\"].mean() * 100\n",
    "\n",
    "    top_subs = df[\"subreddit\"].value_counts().head(top_n)\n",
    "\n",
    "    return {\n",
    "        \"rows\": total,\n",
    "        \"dt_min\": dt_min,\n",
    "        \"dt_max\": dt_max,\n",
    "        \"us_pct\": us_pct,\n",
    "        \"top_subreddits\": top_subs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ad876287-6f64-4014-97f1-0e38a05789cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== event1_kyiv ===\n",
      "Rows: 2940\n",
      "Date range: 2022-02-20 12:52:49 → 2022-03-19 23:01:46\n",
      "Percent U.S.-focused: 0.00%\n",
      "\n",
      "Top subreddits:\n",
      "subreddit\n",
      "InvasionOfUkraine    1700\n",
      "RussiaReplacement     664\n",
      "RussiaDenies          306\n",
      "GTAorRussia           185\n",
      "UkraineWarFootage      69\n",
      "UkrainePics            16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== event2_kherson ===\n",
      "Rows: 795\n",
      "Date range: 2022-10-15 00:00:33 → 2022-11-30 17:18:46\n",
      "Percent U.S.-focused: 0.00%\n",
      "\n",
      "Top subreddits:\n",
      "subreddit\n",
      "RussiaDenies            214\n",
      "GTAorRussia             166\n",
      "volunteersForUkraine    146\n",
      "ArtForUkraine           118\n",
      "UkraineWarReports        89\n",
      "InvasionOfUkraine        23\n",
      "UkraineWarFootage        14\n",
      "UkrainePics              12\n",
      "LiveUkraine              11\n",
      "RussiaReplacement         2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== event3_stalemate ===\n",
      "Rows: 9955\n",
      "Date range: 2023-03-01 00:23:08 → 2023-06-29 23:17:35\n",
      "Percent U.S.-focused: 0.00%\n",
      "\n",
      "Top subreddits:\n",
      "subreddit\n",
      "WagnerVsRussia          4809\n",
      "UkraineLosses           2626\n",
      "volunteersForUkraine    1216\n",
      "GTAorRussia              242\n",
      "RussiaDenies             223\n",
      "ukraina                  158\n",
      "UkraineOSINT             136\n",
      "UkraineWarReports        126\n",
      "UkraineWarFootage        118\n",
      "ArtForUkraine            117\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== event4_trump_election ===\n",
      "Rows: 359088\n",
      "Date range: 2024-10-25 00:00:06 → 2024-11-19 23:59:57\n",
      "Percent U.S.-focused: 26.29%\n",
      "\n",
      "Top subreddits:\n",
      "subreddit\n",
      "UkraineWarVideoReport    65347\n",
      "worldnews                62865\n",
      "UkraineRussiaReport      48270\n",
      "UkrainianConflict        29876\n",
      "ukraine                  28350\n",
      "europe                   23691\n",
      "AskARussian              18897\n",
      "CombatFootage            17711\n",
      "politics                 17168\n",
      "conspiracy                7935\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== event5_white_house_meeting ===\n",
      "Rows: 241262\n",
      "Date range: 2025-02-15 00:00:04 → 2025-03-08 12:23:23\n",
      "Percent U.S.-focused: 32.53%\n",
      "\n",
      "Top subreddits:\n",
      "subreddit\n",
      "worldnews                45346\n",
      "europe                   30335\n",
      "UkraineWarVideoReport    29498\n",
      "UkraineRussiaReport      26287\n",
      "ukraine                  24023\n",
      "UkrainianConflict        21196\n",
      "politics                 17020\n",
      "AskARussian               8576\n",
      "neoliberal                5221\n",
      "geopolitics               4414\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "summaries = {}\n",
    "\n",
    "for event, path in event_paths.items():\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"{event}: file not found (0 rows extracted)\")\n",
    "        continue\n",
    "\n",
    "    s = summarize_event_csv(path, top_n=10)\n",
    "    summaries[event] = s\n",
    "\n",
    "    print(f\"\\n=== {event} ===\")\n",
    "    print(\"Rows:\", s[\"rows\"])\n",
    "    print(\"Date range:\", s[\"dt_min\"], \"→\", s[\"dt_max\"])\n",
    "    if s[\"us_pct\"] is not None:\n",
    "        print(f\"Percent U.S.-focused: {s['us_pct']:.2f}%\")\n",
    "    print(\"\\nTop subreddits:\")\n",
    "    print(s[\"top_subreddits\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff4234-917b-4ea4-bfc7-7da8486cbd50",
   "metadata": {},
   "source": [
    "## 8. Save Extraction Summary \n",
    "\n",
    "This exports a lightweight summary table that can be referenced in the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c2536ab3-3555-4b1f-9b40-3355915bbd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>rows</th>\n",
       "      <th>dt_min</th>\n",
       "      <th>dt_max</th>\n",
       "      <th>us_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>event1_kyiv</td>\n",
       "      <td>2940</td>\n",
       "      <td>2022-02-20 12:52:49</td>\n",
       "      <td>2022-03-19 23:01:46</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>event2_kherson</td>\n",
       "      <td>795</td>\n",
       "      <td>2022-10-15 00:00:33</td>\n",
       "      <td>2022-11-30 17:18:46</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>event3_stalemate</td>\n",
       "      <td>9955</td>\n",
       "      <td>2023-03-01 00:23:08</td>\n",
       "      <td>2023-06-29 23:17:35</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>event4_trump_election</td>\n",
       "      <td>359088</td>\n",
       "      <td>2024-10-25 00:00:06</td>\n",
       "      <td>2024-11-19 23:59:57</td>\n",
       "      <td>26.287150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>event5_white_house_meeting</td>\n",
       "      <td>241262</td>\n",
       "      <td>2025-02-15 00:00:04</td>\n",
       "      <td>2025-03-08 12:23:23</td>\n",
       "      <td>32.528952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        event    rows              dt_min              dt_max  \\\n",
       "0                 event1_kyiv    2940 2022-02-20 12:52:49 2022-03-19 23:01:46   \n",
       "1              event2_kherson     795 2022-10-15 00:00:33 2022-11-30 17:18:46   \n",
       "2            event3_stalemate    9955 2023-03-01 00:23:08 2023-06-29 23:17:35   \n",
       "3       event4_trump_election  359088 2024-10-25 00:00:06 2024-11-19 23:59:57   \n",
       "4  event5_white_house_meeting  241262 2025-02-15 00:00:04 2025-03-08 12:23:23   \n",
       "\n",
       "      us_pct  \n",
       "0   0.000000  \n",
       "1   0.000000  \n",
       "2   0.000000  \n",
       "3  26.287150  \n",
       "4  32.528952  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_rows = []\n",
    "\n",
    "for event, s in summaries.items():\n",
    "    summary_rows.append({\n",
    "        \"event\": event,\n",
    "        \"rows\": s[\"rows\"],\n",
    "        \"dt_min\": s[\"dt_min\"],\n",
    "        \"dt_max\": s[\"dt_max\"],\n",
    "        \"us_pct\": s[\"us_pct\"],\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"event\")\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "83ba3b7c-c428-4740-aca9-8344db001fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/processed/event_extraction_summary.csv'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_out = os.path.join(output_dir, \"event_extraction_summary.csv\")\n",
    "summary_df.to_csv(summary_out, index=False)\n",
    "summary_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b2ec7-035d-40da-ae30-dda317a4dab0",
   "metadata": {},
   "source": [
    "## 9. Notes / Next Steps\n",
    "\n",
    "Next notebooks will use the extracted event CSVs:\n",
    "\n",
    "- `02_exploration.ipynb`  \n",
    "  Quick inspection and sanity checks per event\n",
    "\n",
    "- `03_sentiment.ipynb`  \n",
    "  VADER sentiment analysis (overall + U.S.-focused comparison)\n",
    "\n",
    "- `04_topics.ipynb`  \n",
    "  Topic modeling per event window (LDA or similar)\n",
    "\n",
    "The Streamlit dashboard will later load these per-event CSVs and allow interactive comparisons,\n",
    "including U.S.-focused vs non-U.S. subreddit discussions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
